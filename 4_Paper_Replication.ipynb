{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqw1O1SEBhrZN+jZmkK5DH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaiSuryaPrabu/deep_learning/blob/main/4_Paper_Replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Architecture"
      ],
      "metadata": {
        "id": "5NV1MGO0ngeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper : https://arxiv.org/pdf/2010.11929.pdf"
      ],
      "metadata": {
        "id": "WLxuOC-Lob2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Collection of **layers** are called as **blocks**\n",
        "* Collection of **blocks** makes the model architecture\n"
      ],
      "metadata": {
        "id": "r8bkexLhnk0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The main resource in the paper \n",
        "* The architecture image in **Fig 1**\n",
        "* The mathematical equations in the **Section 3.1**\n",
        "* The hyperparameter tuning in the **table 1**"
      ],
      "metadata": {
        "id": "0NC4YHvfoS-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stages "
      ],
      "metadata": {
        "id": "DtffxKw9qYvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Input\n",
        "* Turn the input image into patches\n",
        "* And number the patches\n",
        "### 2. Embedded Patches\n",
        "* Embedding is used to convert the *images* into *vectors*\n",
        "### 3. Layer Normalization\n",
        "* Called as **Norm**\n",
        "* Regularizing the neural network *(Reducing the overfitting)*\n",
        "* `torch.nn.LayerNorm()` is used to get **norm**\n",
        "### 4. Multi Head Attention - MSA\n",
        "### 5. MLP\n",
        "* A Multi Layer Perceptron block contains\n",
        "    1. `nn.Linear()` * 2 times\n",
        "    2. `nn.GELU()` * 1 times\n",
        "    3. `nn.Dropout()` * 1 times\n",
        "### MLP Head\n",
        "* This is the **output layer**\n",
        "* This is the **classifier head**\n"
      ],
      "metadata": {
        "id": "_rLayfnwvqJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Equations"
      ],
      "metadata": {
        "id": "Y7Vn_ogQvs0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Equation 1\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}\n",
        "\\end{aligned}\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ClYlDgOFvwui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This equation deals with the input image of\n",
        "* Class tokens\n",
        "* Patch embeddings\n",
        "* Position embeddings\n",
        "> **E** means Embedding\n",
        "\n",
        "In the vector form it looks like\n",
        "`input_image = [class_token,image_patch_1,image_patch_2,...]`"
      ],
      "metadata": {
        "id": "TD6c6FsyxXPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Equation 2\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MLie6N8RyUVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{aligned}\n",
        "\\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)\n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "yHwpSSxnyV3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It tells that for each layer from 1 to â„“ contains the **MSA** layer and the **Norm** \n",
        "* The $+$ is the **residual connection**\n",
        "* Psudeo code\n",
        "    * `output_msa_block = MSA_layer(Norm_layer(x_input)) + x_input`"
      ],
      "metadata": {
        "id": "93l-T3kPyxaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Equation 3\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L \\\\\n",
        "\\end{aligned}\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "10-gDXFt0FWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Same as the equation 2\n",
        "* Pseudo code\n",
        "    * `output_mlp_block = MLP_layer(Norm_layer(output_msa_block)) + output_msa_block`"
      ],
      "metadata": {
        "id": "37k9c9cS0OMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Equation 4\n",
        "\\begin{aligned}\n",
        "\\mathbf{y} &=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) & &\n",
        "\\end{aligned}\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cf3g5sdK0kAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This is the last layer **L**\n",
        "* This layer is wrapped by **Norm layer**\n",
        "* The pseudo code\n",
        "    * `y = linear_layer(Norm_layer(output_mlp_block[0]))`"
      ],
      "metadata": {
        "id": "ZH1rGBih0snP"
      }
    }
  ]
}